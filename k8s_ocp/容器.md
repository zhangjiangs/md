## 容器实现原理

现今最热门的服务器技术。

容器到底是怎么一回事儿？

容器其实就是一种沙盒技术，像集装箱一样把你的应用“装”起来，这样应用和应用之间就因为又了边界而不至于相互干扰，而被装进沙盒的应用也可以被方便的搬来搬去，这其实也是paas最理想的状态。

# 1、什么是进程

一个程序运行起来之后的计算机执行环境的总和就是进程。

而容器技术的核心功能就是通过约束和修改进程的动态表现，为其创造一个"边界"。

对于docker等大多数Linux容器来说，Cgroups技术是用来制造约束的主要手段，而Namespace技术是用来修改进程视图的主要方法。

# 2、Namespace——隔离

**namespace 是 Linux 内核用来隔离内核资源的方式。**

通过 namespace 可以让一些进程只能看到与自己相关的一部分资源，而另外一些进程也只能看到与它们自己相关的资源，这两个进程根本就感觉不到对方的存在。具体的实现方式是把一个或多个进程的相关资源指定在同一个 namespace 中。

Linux namespaces 是对全局系统资源的一种封装隔离，使得处于不同 namespace 的进程拥有独立的全局系统资源，改变一个 namespace 中的系统资源只会影响当前 namespace 里的进程，对其他 namespace 中的进程没有影响。

====================================== ===================================

在Linux中创建线程的系统调用是 clone()，当用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值。

Linux 操作系统还提供了 PID、Mount、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行隔离操作。

比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。

这，就是 Linux 容器最基本的实现原理了。所以说，容器，其实是一种特殊的进程而已。

**总结：想想虚拟机和容器的区别**

虚拟化软件是虚拟机最主要的部分（hypervisor、Citrix XenServer、VMware workstation、Oracle virtual box、Linux kvm）。它们通过硬件虚拟化功能，模拟出了运行一个操作系统需要的各种硬件，比如 CPU、内存、I/O 设备等等。然后，它在这些虚拟的硬件上安装了一个新的操作系统，即 Guest OS。

这样，用户的应用进程就可以运行在这个虚拟的机器中，它能看到的自然也只有 Guest OS 的文件和目录，以及这个机器里的虚拟设备。这就是为什么虚拟机也能起到将不同的应用进程相互隔离的作用。

而在容器技术里用容器软件（ Docker Engine）代替虚拟化软件，这也是为什么，很多人会把容器技术称为“轻量级”虚拟化技术的原因，实际上就是把虚拟机的概念套在了容器上。

但是在理解了 Namespace 的工作方式之后，我们发现跟真实存在的虚拟机不同，在使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace 参数。这时，这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和文件，只能访问到各自 Network Namespace 里的网络设备，就仿佛运行在一个个“容器”里面，与世隔绝。

虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用，此外，用户应用运行在虚拟机里面，它对宿主机资源调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。

而相比之下，容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。

不过，有利就有弊，基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处，其中最主要的问题就是：**隔离得不彻底。**

**首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。**

**其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。**

# 3、Cgroups——限制

为什么用Namespace对进程做了隔离以后还要做限制？

虽然容器内的第 1 号进程只能看到容器里的情况，但是宿主机上，它其他所有进程之间依然是平等的竞争关系。这就意味着，它进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。当然，它自己也可能把所有宿主机资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。

而**Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。**

此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。在今天的分享中，我们只关注其对容器的“限制”能力。

在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。

除 CPU 子系统外，Cgroups 的每一项子系统都有其独有的资源限制能力，比如：

blkio，为块设备设定I/O 限制，一般用于磁盘等设备；
cpuset，为进程分配单独的 CPU 核和对应的内存节点；
memory，为进程设定内存使用的限制。
它就是一个子系统目录加上一组资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。

而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令：

```
$ docker run -it --cpu-period=100000 --cpu-quota=20000 centos /bin/sh
```

这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。

通过以上讲述，一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups 配置的限制。

这也是容器技术中一个非常重要的概念，即：容器是一个“单进程”模型。



# 4、rootfs——容器镜像

思考一个问题，我们通过Namespace对进程进行视图隔离，通过Cgroup对资源进行约束限制，但是为什么我们在容器中看到的文件结构和宿主机操作系统的一致，但是却不是宿主机的文件目录呢？

所以跟 Namespace 的情况类似，Cgroups 对资源的限制能力也有很多不完善的地方。

造成这个问题的原因就是， 文件系统并不知道用户通过Namespace和Cgroups对这个容器做了什么样的限制。


因此，这个时候需要chroot命令，change root file system，改变进程根目录到指定位置。

Mount  Namespace正是基于不断改进的chroot命令被发明出来的，也是Linux下的第一个Namespace。

为了让这个容器看起来更真实，一般会在容器的根目录下挂载一个完整的Linux文件系统。

这个挂载在容器根目录上用来为容器进程提供隔离后执行环境的文件系统就是所谓的“容器镜像”，还有个更专业的名字：rootfs（根文件系统）

现在应该可以理解：docker项目最核心的原理实际上就是为待创建的用户进程：

启用Linux Namespace配置；

设置指定的Cgroups参数；

切换进程的根目录（change root）。

这样一个完整的容器就诞生了。

另外，需要明确的是，rootfs只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。

实际上同一台机器上所有容器都共享宿主机操作系统内核，这就意味着应用程序如果需要配置内核参数，加载额外的内核模块，或者需要直接和内核交互的时候，这些操作都将是全局性的，牵一发而动全身。

这也是容器相比于虚拟机的主要缺陷之一。

不过正是由于rootfs的存在，容器才有了一个非常重要的特性：一致性。

如何有效的更新、升级镜像？

镜像升级和更新存在的问题：在制作rootfs的时候每做一次操作更新就要保存一个rootfs，但是麻烦的是每做一次修改，两个rootfs之间就没有任何联系了，最终导致管理繁琐。

docker 使用overlay文件系统来构建和管理镜像与容器的磁盘结构。docker在实现镜像的时候并没有沿用制作rootfs的流程，而是在设计中引入了**层（layer）**的概念，也就是说用户制作镜像的每一步操作都会生成一个层，也就是一个增量rootfs。

这种设计主要基于“联合文件挂载系统”的技术：在centos中docker使用的是overlay2的文件系统。

而在Ubantu和Debian发行版的Linux系统里也会用到AuFS文件系统。因为AuFS至今还未进入Linux内核。

overlay文件系统分为lowerdir、upperdir、merged，其中lower目录可以是多个，work目录为工作基础目录，挂载后内容会被清空，且在使用过程中其内容用户不可见， 对外统一展示为merged，uperdir和lower的同名文件会被upperdir覆盖。

```shell
mount -t overlay overlay -o lowerdir=./a,upperdir=./b,workdir=./c ./merged
```

注：workdir必须和upperdir是mount在同一个文件系统下， 而lower不是必须的

Docker镜像使用的rootfs往往是由多个层组成，每个层就是一个增量rootfs，而在使用镜像的时候docker会把这些增量联合挂载在一个统一的挂载点上。

容器的rootfs结构图：

![img](https://preview.cloud.189.cn/image/imageAction?param=14CD14D4A5C475A058CB05BC8CF44CBEAC5EA217192B1B7794D507091E02492D0671979C2A9AEA42D1471E02D15CF97A61B673F8ABC202B4AA746687F7371A025ABBD544A5CF49F7C49873389422E57F5774EBDA3118924F236DDF76FCCC66771EAC70F7D9874A87F09A5C039DD4A2BD)

## 4.1、只读层（ro+wh）(read only+whiteout)

只读层对应的就是容器镜像的层，挂载方式都是只读的形式，都以增量的方式分别包含了操作系统 文件结构的一部分。

## 4.2、可读写层（rw）

这是容器rootfs的最上面一层，挂载方式读写，在写入文件之前这个目录是空的，而一旦在容器里做了读写操作，修改的内容就会以增量的方式出现在该层中。

这一层就是专门用来存放修改rootfs后产生的增量的，无论是增、删、改都发生在这一层，而当我们使用完这个修改过的容器后还可以使用docker commit和push命令保存这个修改过的可读写层，并上传到镜像仓库供他人使用，与此同时，原来的只读层里的内容不会由任何变化，这就是rootfs的好处。

## 4.3、init层

init层是一个以-init结尾的层，在只读层和可读写层之间，init层是docker项目单独生成的一个内部层，专门用来存放/etc/hosts、/etc/resolv.conf等信息。

最终容器镜像中所有的层组成Lower层+init层+可读写层被联合挂载到相应的目录下，表现为一个完整的操作系统给容器使用。

# 5、dockerfile

在工作中，将应用容器化的第一步是制作容器镜像，相比较前面介绍的制作rootfs的过程，docker提供了一个非常便捷的方式：dockerfile。

dockerfile的设计思想是使用一些标准的**原语**（大写、高亮的词语），描述所要构建的dockers镜像，并且这些原语都是按照顺序处理的。

为了指定基本映像，第一条指令必须是*FROM*。一个声明以`＃`字符开头则被视为注释。可以在Docker文件中使用`RUN`，`CMD`，`FROM`，`EXPOSE`，`ENV`等指令。

需要注意的是dockerfile里每个指令执行以后都会生成一个对应的镜像层，即使原指令并没有明显修改文件的操作，它对应的层也会存在，只不过在外界看来这一层是空的。

注意：dockerfile最多只能写128行，想想为什么？

# 6、Volume——数据卷

构建出了和宿主机完全隔离的文件系统环境，这时考虑两个问题：

宿主机如何获取容器里进程新建的文件？

容器里的进程怎么才能访问到宿主机上的文件和目录？

这正是docker volume要解决的问题。volume机制允许我们将宿主机上指定的目录或者文件挂在到容器中进行读取和修改。

volume的声明方式：

```
docker run -v /test ....
docker run -v /home:/test ...
```

这两种声明方式本质上都是一样的：都是把宿主机的一个目录挂载进容器的test目录。

只不过第一种情况由于没有声明宿主机的目录，因此docker会默认在宿主机上新建一个临时目录/var/lib/docker/volumes/[volume_ID]/_data，然后将其挂载到容器的test目录下。

这里用到的挂载技术是Linux的**绑定挂载**（bind mount）机制。

它的主要作用是允许将一个目录或者文件而不是整个设备挂载到指定目录上，并且这是你在该挂载点上进行的任何操作都只是发生在被挂载的目录或者文件上，而原挂载点的内容会被隐藏起来且不受影响。

比如：将/home目录挂载到/test目录上，实际修改的是home目录的inode（存放文件内容的一个“对象”，绑定挂载在Linux内核中实际上是一个inode替换的过程）。当我们一旦执行了umount命令后，/test目录原来的内容就会恢复，因为修改操作实际发生在home目录里。

因此进程在容器里对/test这个目录的所有操作都实际发生在宿主机的对应目录中，而不会影响容器镜像的内容。所以对这个目录下的所有操作都不会被docker commit提交！但是这个挂载点目录/test会出现在新的容器镜像中（因为/test目录是被挂载在容器的rootfs可读写层）。

